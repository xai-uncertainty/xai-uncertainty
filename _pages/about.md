---
layout: about
title: About
permalink: /
subtitle: 
news: False  # includes a list of news items
latest_posts: False  # includes a list of the newest posts
selected_papers: False # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

<p align="center" style="font-weight:bold; font-size:30px"> Uncertainty meets Explainability | Workshop and Tutorial @
<a href="https://2023.ecmlpkdd.org/">ECML-PKDD 2023</a>
</p>

---

<p style="color:blue" align="center"> Click
<a href="https://cmt3.research.microsoft.com/ECMLPKDDworkshop2023/Track/3/Submission/Create">[here]</a>  to submit a paper or 
<a href="cfp">[here]</a> to read the CFP
</p>

---

<p style="color:blue" align="center"> 
<a href="mailto:xai-uncertainty@gmail.com">[Click here to submit any question to the organizers]</a></p>

---

Machine learning systems have become increasingly popular in crucial high-stakes fields, such as healthcare and finance. To be effective in these domains, the models must not only make precise predictions but also provide relevant explanations for those predictions. To achieve this goal, there has been a substantial research effort in recent years to develop techniques that explain black-box models and create models that are interpretable by design.

Simultaneously, there is a growing emphasis on machine learning models that account for uncertainty. Decision-making systems can encounter uncertainties stemming from different origins, each offering a distinct perspectives. For instance, aleatoric uncertainty arises from the inherent randomness of the prediction, while epistemic uncertainty from the insufficient amount of data. In general, incorporating uncertainty enhances a model's reliability by allowing it to acknowledge scenarios where it lacks the necessary knowledge to make an accurate prediction.

The intersection of explainability and uncertainty has drawn attention for its potential to combine these domains towards Trustworthy ML. Some notable innovative approaches include: developing interpretability methods for probabilistic models, qunatifying the uncertainty of explanations, and explaining the sources of uncertainty.

The primary goal of this full-day event, consisting of a Tutorial and a Workshop, is to jointly explore how explainability and uncertainty can be leveraged to build robust and trustworthy AI systems. The tutorial (morning session) will establish the foundation for (a) uncertainty modelling and (b) explainability in machine learning. Then, the workshop (afternoon session) will delve into innovative techniques at the intersection of these two domains.

The workshop & tutorial will be an in-person event at [ECML-PKDD 2023](https://2023.ecmlpkdd.org/). The session will cover invited talks, contributed talks, posters, and a panel discussion.

---

### **Key Dates**

- Submission Deadline: **12 June 2023 (AoE)**
- Author Notification: **12 July 2023 (AoE)**
- Event Date: **TBA (18th or 22nd of September 2023)**

---

### **Organizers**

- Tutorial:
  - [Viktor Bengs](https://www.kiml.ifi.lmu.de/people/postdocs/bengs/index.html)
  - [Eyke Hullermeier](https://scholar.google.com/citations?user=usVJeNN3xFAC&hl=en)
  - [Willem Waegeman](https://scholar.google.com/citations?user=jdjZppMAAAAJ&hl=en)
- Workshop:
  - [Vasilis Gkolemis](https://givasile.github.io)
  - [Christos Diou](https://diou.github.io)

---

### **Program Committee**

TBA

---

### **Organizations**

<img src="assets/img/ghent_logo.png" width="350">
<img src="assets/img/LMU_logo.svg" width="350">
<img src="assets/img/harokopio.png" width="500">
<img src="assets/img/athena_logo.jpg" width="350">

<!-- Associating a model’s predictions with an uncertainty level is a form of transparency; the user is informed to what extent they should trust the system’s decision. However, uncertainty is not sufficient for understanding how a black box ML model predicts. In high-stakes applications, there is a need for methods that do both; predict with uncertainty and explain their predictions. Furthermore, studies have shown that explainability methods are also vulnerable to inconsistencies and instabilities, which poses a significant challenge to their reliability. Therefore, the explanations should also account for uncertainty to express to what extent the stakeholder should trust them.  -->

<!-- The workshop aims to attract submissions from renowned experts in the fields of Explainability, Statistics and Machine Learning. Application-specific works in cases where decision making implies stable explainability methods are welcomed. -->
