---
layout: about
title: about
permalink: /
subtitle: 
news: False  # includes a list of news items
latest_posts: False  # includes a list of the newest posts
selected_papers: False # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---



---

<p style="color:blue" align="center"> Papers should be submitted at 
<a href="cmt3.research.microsoft.com">cmt3.research.microsoft.com</a>
</p>

---

<p style="color:blue" align="center"> Please submit any question
<a href="mailto:xai-uncertainty@gmail.com">here</a></p>

---

#### **Combined Event - Workshop and Tutorial @ ECML-PKDD 2023**

---

Recently, critical high-stakes domains such as healthcare and finance have widely embraced the use of machine learning systems. In these domains, the focus is not solely on models that predict accurately but also on models that can provide explanations for their predictions. This requirement is mandated by various official regulations, such as GDPR, which clearly states that individuals have right in "meaningful explanations of the logic involved" when any type of automated process participates in a decision.

At the same time, there is an increasing focus on models that can incorporate uncertainty into their predictions. Decision-making systems may encounter uncertainty from various sources, each providing unique insights to stakeholders. Aleatoric uncertainty, for example, stems from the inherent randomness of the predicted quantity, while epistemic uncertainty arises due to the limited amount of data available for the model's training. In general, incorporating uncertainty makes a model more reliable since it can acknowledge situations where it lacks knowledge about the correct prediction.

The main objective of this full-day event (workshop and tutorial) is to jointly examine explainability and uncertainty for the development of robust and trustworthy AI systems. The Tutorial will serve as a means for providing the foundation of uncertainty modelling in machine learning, and afterwards the workshop will focus on the value of uncertainty in model interpretability methods.

The event will be in-person event at ECML-PKDD 2023. The session will cover invited talks, contributed talks, posters, and a panel discussion.

### **Key Dates**

- Submission Deadline: 12 June 2023 (AoE)
- Author Notification: 12 July 2023 (AoE)

### **Program Chairs**

- Workshop:
  - Vasilis Gkolemis
  - Christos Diou
- Tutorial:
  - Viktor Bengs
  - Eyke Hullermeier
  - Willem Waegeman

### **Organization Committee**

TBA

### **Organizations**

TBD


<!-- Associating a model’s predictions with an uncertainty level is a form of transparency; the user is informed to what extent they should trust the system’s decision. However, uncertainty is not sufficient for understanding how a black box ML model predicts. In high-stakes applications, there is a need for methods that do both; predict with uncertainty and explain their predictions. Furthermore, studies have shown that explainability methods are also vulnerable to inconsistencies and instabilities, which poses a significant challenge to their reliability. Therefore, the explanations should also account for uncertainty to express to what extent the stakeholder should trust them.  -->

<!-- The workshop aims to attract submissions from renowned experts in the fields of Explainability, Statistics and Machine Learning. Application-specific works in cases where decision making implies stable explainability methods are welcomed. -->
