---
layout: about
title: About
permalink: /
subtitle: 
news: False  # includes a list of news items
latest_posts: False  # includes a list of the newest posts
selected_papers: False # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---



---

<p style="color:blue" align="center"> Papers should be submitted at
<a href="https://cmt3.research.microsoft.com/ECMLPKDDworkshop2023/Track/3/Submission/Create">https://cmt3.research.microsoft.com/ECMLPKDDworkshop2023/Track/3/Submission/Create</a>
</p>

---

<p style="color:blue" align="center"> Please submit any question
<a href="mailto:xai-uncertainty@gmail.com">here</a></p>

---

#### **Combined Event - Workshop and Tutorial @ [ECML-PKDD 2023](https://2023.ecmlpkdd.org/)**

---

Recently, critical high-stakes domains such as healthcare and finance have widely embraced the use of machine learning systems. In these domains, it is not only required that the models predict accurately but also that they can accompany their predictions with meaningful explanations. In the past few years, there is a significant research effort towards methods that explain black-box models, as well as on developing accurate interpretable-by-design models.

At the same time, there is an increasing focus on models that incorporate uncertainty. Decision-making systems may encounter uncertainty from various sources, each providing unique insights to stakeholders. Aleatoric uncertainty, for example, stems from the inherent randomness of the predicted quantity, while epistemic uncertainty arises due to the limited amount of data available for the model's training. In general, incorporating uncertainty makes a model more reliable since it can acknowledge situations where it lacks knowledge about the correct prediction.

Recent developments have demonstrated significant potential for innovation on the intersection of these two areas. For example, on (a) modelling the uncertainty of the explanation
(b) post-hoc uncertainty explanation, i.e., explain the sources of uncertainty and (c) develop interpretability methods in probabilistic models.

The main objective of this full-day event (workshop and tutorial) is to jointly examine explainability and uncertainty for the development of robust and trustworthy AI systems. The Tutorial will serve as a means for providing the foundation of (a) uncertainty modelling and (b) explainability in machine learning; afterwards, the workshop will explore novel methods on the intersection of these two areas.

The workshop & tutorial will be an in-person event at [ECML-PKDD 2023](https://2023.ecmlpkdd.org/). The session will cover invited talks, contributed talks, posters, and a panel discussion.

---

### **Key Dates**

- Submission Deadline: **12 June 2023 (AoE)**
- Author Notification: **12 July 2023 (AoE)**

---

### **Organizers**

- Workshop:
  - [Vasilis Gkolemis](https://givasile.github.io)
  - [Christos Diou](https://diou.github.io)
- Tutorial:
  - [Viktor Bengs](https://www.kiml.ifi.lmu.de/people/postdocs/bengs/index.html)
  - [Eyke Hullermeier](https://scholar.google.com/citations?user=usVJeNN3xFAC&hl=en)
  - [Willem Waegeman](https://scholar.google.com/citations?user=jdjZppMAAAAJ&hl=en)

---

### **Program Committee**

TBA

---

### **Organizations**

<img src="assets/img/harokopio.png" width="600">


todo: add logos for ATHENA, HAROKOPIO, LMU 


<!-- Associating a model’s predictions with an uncertainty level is a form of transparency; the user is informed to what extent they should trust the system’s decision. However, uncertainty is not sufficient for understanding how a black box ML model predicts. In high-stakes applications, there is a need for methods that do both; predict with uncertainty and explain their predictions. Furthermore, studies have shown that explainability methods are also vulnerable to inconsistencies and instabilities, which poses a significant challenge to their reliability. Therefore, the explanations should also account for uncertainty to express to what extent the stakeholder should trust them.  -->

<!-- The workshop aims to attract submissions from renowned experts in the fields of Explainability, Statistics and Machine Learning. Application-specific works in cases where decision making implies stable explainability methods are welcomed. -->
